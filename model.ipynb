{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from mlflow.models import infer_signature\n",
    "import  mlflow\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from dataprep.eda import create_report\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer , SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "import umap.umap_ as umap\n",
    "import prince  # Multiple Correspondence Analysis\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings\n",
    "import missingno as msno\n",
    "from feature_engine.outliers import OutlierTrimmer\n",
    "from category_encoders import TargetEncoder, WOEEncoder, CatBoostEncoder\n",
    "import sweetviz as sv\n",
    "from datetime import datetime\n",
    "import dataprep.eda as dpeda\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import statsmodels.api as sm\n",
    "import AdvancedRelationshipAnalysis \n",
    "import json\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    StratifiedKFold, \n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e0c532-5a5c-4c35-bb3d-24bbeb66be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEDA:\n",
    "    def __init__(self, df, target_col=None):\n",
    "        \"\"\"\n",
    "        Initialize Advanced EDA class\n",
    "\n",
    "        Parameters:\n",
    "        df: pandas DataFrame\n",
    "        target_col: str, target column name for supervised analysis\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "    def basic_feature_cleaning(\n",
    "        self,\n",
    "        nan_threshold=0.5,\n",
    "        correlation_threshold=0.95,\n",
    "        variance_threshold=0.01,\n",
    "        outlier_method='IQR',\n",
    "        dominant_category_threshold=0.95,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Nettoyage des colonnes avancé avec suppression des colonnes inutiles ou redondantes.\n",
    "        \"\"\"\n",
    "        df_cleaned = self.df\n",
    "        print(df_cleaned.columns)\n",
    "        print(len(df_cleaned.columns))\n",
    "        \n",
    "        num_cols = [col for col in self.numeric_cols if col in df_cleaned.columns]\n",
    "\n",
    "        # # 7. Suppression des outliers (IQR ou Z-score)\n",
    "        # if outlier_method == 'IQR':\n",
    "        #     if verbose:\n",
    "        #         print(\"Suppression des outliers avec la méthode IQR...\")\n",
    "        #     Q1 = df_cleaned[num_cols].quantile(0.25)\n",
    "        #     Q3 = df_cleaned[num_cols].quantile(0.75)\n",
    "        #     IQR = Q3 - Q1\n",
    "        #     lower_bound = Q1 - 1.5 * IQR\n",
    "        #     upper_bound = Q3 + 1.5 * IQR\n",
    "        #     df_cleaned = df_cleaned[~((df_cleaned[num_cols] < lower_bound) | (df_cleaned[num_cols] > upper_bound)).any(axis=1)]\n",
    "        \n",
    "        # elif outlier_method == 'Z-score':\n",
    "        #     if verbose:\n",
    "        #         print(\"Suppression des outliers avec la méthode Z-score...\")\n",
    "        #     from scipy.stats import zscore\n",
    "        #     z_scores = np.abs(zscore(df_cleaned[num_cols]))\n",
    "        #     df_cleaned = df_cleaned[(z_scores < 3).all(axis=1)]\n",
    "        # # 1. Supprimer les colonnes contenant \"id\" dans leur nom\n",
    "        if verbose:\n",
    "            print(\"Suppression des colonnes contenant 'id' dans leur nom...\")\n",
    "        id_cols = [col for col in df_cleaned.columns if 'id' in col.lower()]\n",
    "        print(id_cols)\n",
    "        df_cleaned = df_cleaned.drop(columns=id_cols)\n",
    "\n",
    "        # 2. Supprimer les colonnes avec trop de NaN\n",
    "        if verbose:\n",
    "            print(\"Suppression des colonnes avec trop de valeurs manquantes...\")\n",
    "        nan_cols = df_cleaned.columns[df_cleaned.isnull().mean() > nan_threshold]\n",
    "        df_cleaned = df_cleaned.drop(columns=nan_cols)\n",
    "\n",
    "        # 3. Supprimer les colonnes contenant une seule valeur unique\n",
    "        if verbose:\n",
    "            print(\"Suppression des colonnes avec une seule valeur unique...\")\n",
    "        unique_val_cols = [col for col in df_cleaned.columns if df_cleaned[col].nunique() <= 1]\n",
    "        print(unique_val_cols)\n",
    "        df_cleaned = df_cleaned.drop(columns=unique_val_cols)\n",
    "        unique_val_cols2 = [col for col in df_cleaned.columns if len(list(df_cleaned[col].value_counts())) == 1]\n",
    "        print(unique_val_cols2)\n",
    "        df_cleaned = df_cleaned.drop(columns=unique_val_cols2)\n",
    "\n",
    "        # 4. Supprimer les colonnes catégoriques avec une catégorie dominante\n",
    "        if verbose:\n",
    "            print(\"Suppression des colonnes avec une catégorie dominante...\")\n",
    "        dom_cat_cols = [\n",
    "            col for col in df_cleaned.select_dtypes(exclude=[np.number]).columns\n",
    "            if (df_cleaned[col].value_counts(normalize=True).max() > dominant_category_threshold)\n",
    "        ]\n",
    "        df_cleaned = df_cleaned.drop(columns=dom_cat_cols)\n",
    "\n",
    "        # 5. Suppression des colonnes avec faible variance\n",
    "        if verbose:\n",
    "            print(\"Suppression des colonnes avec faible variance...\")\n",
    "        num_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "        if len(num_cols) > 0:\n",
    "            var_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "            try:\n",
    "                var_selector.fit(df_cleaned[num_cols])\n",
    "                low_variance_cols = num_cols[~var_selector.get_support()]\n",
    "                df_cleaned = df_cleaned.drop(columns=low_variance_cols, errors=\"ignore\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Erreur dans la sélection par variance : {e}\")\n",
    "        num_cols = [col for col in self.numeric_cols if col in df_cleaned.columns]\n",
    "\n",
    "        # 6. Supprimer les colonnes fortement corrélées\n",
    "        if verbose:\n",
    "            print(\"Suppression des colonnes fortement corrélées...\")\n",
    "        corr_matrix = df_cleaned[num_cols].corr().abs()\n",
    "        upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        to_drop = [\n",
    "            column for column in corr_matrix.columns\n",
    "            if any(corr_matrix[column][upper_triangle[:, corr_matrix.columns.get_loc(column)]] > correlation_threshold)\n",
    "        ]\n",
    "        df_cleaned = df_cleaned.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "        num_cols = [col for col in self.numeric_cols if col in df_cleaned.columns]\n",
    "\n",
    "        # # 7. Suppression des outliers (IQR ou Z-score)\n",
    "        # if outlier_method == 'IQR':\n",
    "        #     if verbose:\n",
    "        #         print(\"Suppression des outliers avec la méthode IQR...\")\n",
    "        #     Q1 = df_cleaned[num_cols].quantile(0.25)\n",
    "        #     Q3 = df_cleaned[num_cols].quantile(0.75)\n",
    "        #     IQR = Q3 - Q1\n",
    "        #     lower_bound = Q1 - 1.5 * IQR\n",
    "        #     upper_bound = Q3 + 1.5 * IQR\n",
    "        #     df_cleaned = df_cleaned[~((df_cleaned[num_cols] < lower_bound) | (df_cleaned[num_cols] > upper_bound)).any(axis=1)]\n",
    "        \n",
    "        # elif outlier_method == 'Z-score':\n",
    "        #     if verbose:\n",
    "        #         print(\"Suppression des outliers avec la méthode Z-score...\")\n",
    "        #     from scipy.stats import zscore\n",
    "        #     z_scores = np.abs(zscore(df_cleaned[num_cols]))\n",
    "        #     df_cleaned = df_cleaned[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "        # Mise à jour des colonnes après nettoyage\n",
    "        self.numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_cols = df_cleaned.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Nettoyage des caractéristiques terminé.\")\n",
    "            print(f\"Colonnes restantes : {df_cleaned.columns.tolist()}\")\n",
    "\n",
    "        return df_cleaned\n",
    "    def advanced_feature_selection(\n",
    "        self,\n",
    "        methods=[\"basic\", \"mutual_info\", \"rfe\", \"chi2\"],\n",
    "        nan_threshold=0.5,\n",
    "        correlation_threshold=0.95,\n",
    "        variance_threshold=0.01,\n",
    "        top_k_features=10,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform comprehensive feature selection using multiple techniques.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        methods : list, optional (default=['basic', 'mutual_info', 'rfe', 'chi2'])\n",
    "            List of feature selection methods to apply\n",
    "        nan_threshold : float, optional (default=0.5)\n",
    "            Threshold for dropping columns with too many NaN values\n",
    "        correlation_threshold : float, optional (default=0.95)\n",
    "            Threshold for dropping highly correlated features\n",
    "        variance_threshold : float, optional (default=0.01)\n",
    "            Threshold for dropping low variance features\n",
    "        top_k_features : int, optional (default=10)\n",
    "            Number of top features to select in advanced methods\n",
    "        verbose : bool, optional (default=True)\n",
    "            Whether to print detailed information about feature selection\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with selected features\n",
    "        \"\"\"\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        df_selected = self.df.copy()\n",
    "        target_col = self.target_col\n",
    "    \n",
    "        # Validate target column\n",
    "        if target_col and target_col not in df_selected.columns:\n",
    "            raise ValueError(f\"Target column '{target_col}' not found in DataFrame\")\n",
    "    \n",
    "        # Preprocessing for feature selection methods\n",
    "        def preprocess_data(df_input):\n",
    "            \"\"\"Preprocess data for feature selection methods\"\"\"\n",
    "            # Handle categorical variables\n",
    "            df_processed = pd.get_dummies(df_input, drop_first=True)\n",
    "            \n",
    "            # Separate features and target\n",
    "            if target_col and target_col in df_processed.columns:\n",
    "                X = df_processed.drop(columns=[target_col])\n",
    "                y = df_processed[target_col]\n",
    "            else:\n",
    "                X, y = df_processed, None\n",
    "            \n",
    "            return X, y\n",
    "    \n",
    "        # 1. Basic Feature Selection\n",
    "        if \"basic\" in methods:\n",
    "            # Remove columns with too many NaN values\n",
    "            df_selected = df_selected.loc[:, df_selected.isnull().mean() <= nan_threshold]\n",
    "    \n",
    "            # Remove ID-like columns\n",
    "            id_cols = [col for col in df_selected.columns if \"id\" in col.lower()]\n",
    "            df_selected = df_selected.drop(columns=id_cols, errors='ignore')\n",
    "    \n",
    "            # Remove single-value categorical columns\n",
    "            cat_cols = [col for col in df_selected.select_dtypes(exclude=[np.number]).columns \n",
    "                        if df_selected[col].nunique() <= 1]\n",
    "            df_selected = df_selected.drop(columns=cat_cols, errors='ignore')\n",
    "    \n",
    "            # Remove low-variance numerical columns\n",
    "            num_cols = df_selected.select_dtypes(include=[np.number]).columns\n",
    "            if len(num_cols) > 0:\n",
    "                try:\n",
    "                    var_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "                    var_selector.fit(df_selected[num_cols])\n",
    "                    \n",
    "                    # Identify and remove low variance columns\n",
    "                    low_variance_cols = num_cols[~var_selector.get_support()]\n",
    "                    df_selected = df_selected.drop(columns=low_variance_cols, errors='ignore')\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Variance threshold filtering error: {e}\")\n",
    "    \n",
    "            # Remove highly correlated features\n",
    "            try:\n",
    "                corr_matrix = df_selected.select_dtypes(include=[np.number]).corr().abs()\n",
    "                upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "                \n",
    "                # Find columns to drop based on correlation\n",
    "                to_drop = [column for column in upper.columns \n",
    "                           if any(upper[column] > correlation_threshold)]\n",
    "                df_selected = df_selected.drop(columns=to_drop, errors='ignore')\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Correlation-based feature selection error: {e}\")\n",
    "    \n",
    "        # Prepare data for advanced methods\n",
    "        X, y = preprocess_data(df_selected)\n",
    "        \n",
    "        # Ensure we have features to select from\n",
    "        if X.empty:\n",
    "            if verbose:\n",
    "                print(\"No features left after basic selection!\")\n",
    "            return df_selected\n",
    "    \n",
    "        # Set of features to keep\n",
    "        selected_features = set(X.columns)\n",
    "    \n",
    "        # 2. Mutual Information Feature Selection\n",
    "        if \"mutual_info\" in methods and target_col and y is not None:\n",
    "            try:\n",
    "                # Choose appropriate mutual information method based on target type\n",
    "                mi_func = (mutual_info_classif if y.dtype == 'object' or y.dtype == 'category'\n",
    "                           else mutual_info_regression)\n",
    "                \n",
    "                # Compute mutual information scores\n",
    "                mi_scores = mi_func(X, y)\n",
    "                mi_df = pd.DataFrame({\n",
    "                    \"feature\": X.columns, \n",
    "                    \"mutual_info\": mi_scores\n",
    "                }).sort_values(\"mutual_info\", ascending=False)\n",
    "                \n",
    "                # Select top k features\n",
    "                mi_features = set(mi_df.head(min(top_k_features, len(mi_df)))['feature'])\n",
    "                \n",
    "                # Intersect with existing selected features\n",
    "                selected_features &= mi_features\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Top features by Mutual Information:\")\n",
    "                    print(mi_df.head(min(top_k_features, len(mi_df))))\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Mutual Information selection failed: {e}\")\n",
    "    \n",
    "        # 3. Recursive Feature Elimination (RFE)\n",
    "        if \"rfe\" in methods and target_col and y is not None:\n",
    "            try:\n",
    "                # Choose appropriate estimator based on target type\n",
    "                estimator = (LogisticRegression(max_iter=1000) if y.dtype == 'object' or y.dtype == 'category'\n",
    "                             else LinearRegression())\n",
    "                \n",
    "                # Perform RFE\n",
    "                selector = RFE(estimator, n_features_to_select=min(top_k_features, X.shape[1]))\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                # Get RFE selected features\n",
    "                rfe_features = set(X.columns[selector.support_])\n",
    "                \n",
    "                # Intersect with existing selected features\n",
    "                selected_features &= rfe_features\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"\\nTop features by RFE:\")\n",
    "                    print(X.columns[selector.support_])\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Recursive Feature Elimination failed: {e}\")\n",
    "    \n",
    "        # 4. Chi-Squared Feature Selection\n",
    "        if \"chi2\" in methods and target_col and y is not None:\n",
    "            try:\n",
    "                # Chi-squared only works for classification problems\n",
    "                if y.dtype == 'object' or y.dtype == 'category':\n",
    "                    # Encode target variable\n",
    "                    le = LabelEncoder()\n",
    "                    y_encoded = le.fit_transform(y)\n",
    "                    \n",
    "                    # Scale features\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X)\n",
    "                    \n",
    "                    # Select top k features\n",
    "                    selector = SelectKBest(chi2, k=min(top_k_features, X.shape[1]))\n",
    "                    selector.fit(X_scaled, y_encoded)\n",
    "                    \n",
    "                    # Get chi2 selected features\n",
    "                    chi2_features = set(X.columns[selector.get_support()])\n",
    "                    \n",
    "                    # Intersect with existing selected features\n",
    "                    selected_features &= chi2_features\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(\"\\nTop features by Chi-Squared Test:\")\n",
    "                        print(chi2_features)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"Chi-Squared is not applicable for continuous target variables.\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Chi-Squared selection failed: {e}\")\n",
    "    \n",
    "        # # Finalize feature selection\n",
    "        # final_features = list(selected_features)\n",
    "        \n",
    "        # # Add target column back if it exists\n",
    "        # if target_col:\n",
    "        #     final_features.append(target_col)\n",
    "        \n",
    "        # # Return final dataframe with selected features\n",
    "        # df_final = self.df[final_features]\n",
    "        \n",
    "        # # Update numeric and categorical columns\n",
    "        # self.numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "        # self.categorical_cols = df_final.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        # if verbose:\n",
    "        #     print(\"\\nFinal Selected Features:\")\n",
    "        #     print(final_features)\n",
    "        \n",
    "        # return df_final \n",
    "        # Finalize feature selection\n",
    "        final_features = list(selected_features)\n",
    "        \n",
    "        # Ensure selected features exist in the original DataFrame\n",
    "        final_features = [feature for feature in final_features if feature in self.df.columns]\n",
    "        \n",
    "        # Add target column back if it exists\n",
    "        if target_col and target_col not in final_features:\n",
    "            final_features.append(target_col)\n",
    "        \n",
    "        # Safely slice the DataFrame\n",
    "        try:\n",
    "            df_final = self.df[final_features]\n",
    "        except KeyError as e:\n",
    "            missing_features = list(set(final_features) - set(self.df.columns))\n",
    "            if verbose:\n",
    "                print(f\"Warning: Missing features: {missing_features}\")\n",
    "            final_features = [f for f in final_features if f in self.df.columns]\n",
    "            df_final = self.df[final_features]\n",
    "        \n",
    "        # Update numeric and categorical columns\n",
    "        self.numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_cols = df_final.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nFinal Selected Features:\")\n",
    "            print(final_features)\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "    def simple_statistical_imputation(self):\n",
    "        \"\"\"\n",
    "        Perform simple imputation using:\n",
    "        - mean or median based on Kolmogorov-Smirnov test for numerical variables\n",
    "        - most frequent value for categorical variables\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Imputed dataframe\n",
    "        dict: Dictionary containing the strategy used for each column\n",
    "        \"\"\"\n",
    "        self.df = self.basic_feature_cleaning().copy()\n",
    "        #self.df = self.advanced_feature_selection(methods=[\"basic\", \"rfe\", \"chi2\"])\n",
    "        df_imputed = self.df.copy()\n",
    "        self.numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_cols = df_imputed.select_dtypes(exclude=[np.number]).columns\n",
    "        # imputation_strategies = {}\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        for col in self.numeric_cols:\n",
    "            print(col)\n",
    "            # Get non-null values for the column\n",
    "            non_null_values = self.df[col].dropna()\n",
    "            \n",
    "            if len(non_null_values) > 0:\n",
    "                # Perform Kolmogorov-Smirnov test against normal distribution\n",
    "                _, p_value = stats.kstest(\n",
    "                    stats.zscore(non_null_values), \n",
    "                    'norm'\n",
    "                )\n",
    "                \n",
    "                # Choose strategy based on p-value\n",
    "                if p_value < 0.05:\n",
    "                    # Not normally distributed - use median\n",
    "                    strategy = 'median'\n",
    "                else:\n",
    "                    # Normally distributed - use mean\n",
    "                    strategy = 'mean'\n",
    "                \n",
    "                # Apply imputation\n",
    "                imputer = SimpleImputer(strategy=strategy)\n",
    "                df_imputed[col] = imputer.fit_transform(df_imputed[[col]])\n",
    "                \n",
    "                # # Store the strategy used\n",
    "                # imputation_strategies[col] = {\n",
    "                #     'strategy': strategy,\n",
    "                #     'p_value': p_value,\n",
    "                #     'fill_value': df_imputed[col].iloc[0] if strategy == 'mean' \n",
    "                #                 else df_imputed[col].median(),\n",
    "                #     'missing_count': self.df[col].isnull().sum(),\n",
    "                #     'missing_percentage': (self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "                # }\n",
    "        \n",
    "        # Handle categorical columns with mode imputation\n",
    "        if len(self.categorical_cols) > 0:\n",
    "            for col in self.categorical_cols:\n",
    "                print(col)\n",
    "\n",
    "                # Get mode (most frequent value)\n",
    "                mode_value = self.df[col].mode()[0]\n",
    "                \n",
    "                # Apply mode imputation\n",
    "                df_imputed[col] = self.df[col].fillna(mode_value)\n",
    "                \n",
    "                # Calculate frequency of the most common value\n",
    "                value_counts = self.df[col].value_counts(normalize=True)\n",
    "                most_common_freq = value_counts.iloc[0] if not value_counts.empty else 0\n",
    "                \n",
    "                # Store the strategy and statistics\n",
    "                # imputation_strategies[col] = {\n",
    "                #     'strategy': 'most_frequent',\n",
    "                #     'fill_value': mode_value,\n",
    "                #     'most_common_frequency': most_common_freq * 100,  # as percentage\n",
    "                #     'missing_count': self.df[col].isnull().sum(),\n",
    "                #     'missing_percentage': (self.df[col].isnull().sum() / len(self.df)) * 100,\n",
    "                #     'unique_values': self.df[col].nunique()\n",
    "                # }\n",
    "        \n",
    "        # # Add summary statistics\n",
    "        # imputation_strategies['summary'] = {\n",
    "        #     'total_missing': self.df.isnull().sum().sum(),\n",
    "        #     'total_missing_percentage': (self.df.isnull().sum().sum() / (len(self.df) * len(self.df.columns))) * 100,\n",
    "        #     'columns_with_missing': self.df.isnull().sum()[self.df.isnull().sum() > 0].index.tolist()\n",
    "        # }\n",
    "        \n",
    "        self.df = df_imputed.copy()\n",
    "    \"\"\"\n",
    "    def advanced_missing_data_analysis(self):\n",
    "        Perform advanced missing data analysis and visualization\n",
    "        # Missing data patterns\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Missing data heatmap\n",
    "        missing_matrix = self.df.isnull().astype(int)\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=missing_matrix.T,\n",
    "                x=range(len(missing_matrix)),\n",
    "                y=missing_matrix.columns,\n",
    "                colorscale='RdYlBu',\n",
    "                name='Missing Pattern'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='Missing Data Pattern Analysis',\n",
    "            xaxis_title='Row Index',\n",
    "            yaxis_title='Features',\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Missing data correlation\n",
    "        missing_corr = missing_matrix.corr()\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=missing_corr,\n",
    "            x=missing_corr.columns,\n",
    "            y=missing_corr.columns,\n",
    "            colorscale='RdBu'\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title='Missing Data Correlation Analysis',\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        return missing_corr\n",
    "    \"\"\"\n",
    "    \n",
    "    def advanced_categorical_encoding(self, ordinal_columns=None, save_path='encoders'):\n",
    "        \"\"\"\n",
    "        Encode categorical variables using appropriate encoding methods:\n",
    "        - OneHotEncoder for ordinal variables if specified, otherwise LabelEncoder\n",
    "        - LabelEncoder for all other nominal variables\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ordinal_columns : list, optional\n",
    "            List of column names that should be treated as ordinal variables and encoded with OneHotEncoder\n",
    "        save_path : str, default='encoders'\n",
    "            Directory path to save the encoder models\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame : Encoded dataframe\n",
    "        dict : Dictionary containing encoding information and statistics\n",
    "        \"\"\"\n",
    "        if ordinal_columns is None:\n",
    "            ordinal_columns = []\n",
    "    \n",
    "        # Create directory for saving encoders if it doesn't exist\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "        # Initialize containers\n",
    "        encoding_info = {}\n",
    "        df_encoded = self.df.copy()\n",
    "    \n",
    "        # Process categorical columns\n",
    "        for col in self.categorical_cols:\n",
    "            n_unique = self.df[col].nunique()\n",
    "            value_counts = self.df[col].value_counts()\n",
    "            missing_count = self.df[col].isnull().sum()\n",
    "    \n",
    "            # Store basic column statistics\n",
    "            encoding_info[col] = {\n",
    "                'unique_values': n_unique,\n",
    "                'value_distribution': value_counts.to_dict(),\n",
    "                'missing_values': missing_count,\n",
    "                'missing_percentage': (missing_count / len(self.df)) * 100\n",
    "            }\n",
    "    \n",
    "            # Handle missing values before encoding\n",
    "            if missing_count > 0:\n",
    "                df_encoded[col] = df_encoded[col].fillna('Missing')\n",
    "    \n",
    "            # Determine encoding method based on ordinal_columns list\n",
    "            if col in ordinal_columns:\n",
    "                # Use OneHotEncoder for specified ordinal variables\n",
    "                encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                encoded_data = encoder.fit_transform(df_encoded[[col]])\n",
    "    \n",
    "                # Get feature names after encoding\n",
    "                feature_names = encoder.get_feature_names_out([col])\n",
    "    \n",
    "                # Replace original column with encoded columns\n",
    "                df_encoded = df_encoded.drop(col, axis=1)\n",
    "                encoded_df = pd.DataFrame(\n",
    "                    encoded_data,\n",
    "                    columns=feature_names,\n",
    "                    index=df_encoded.index\n",
    "                )\n",
    "                df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "    \n",
    "                # Store encoding information\n",
    "                encoding_info[col].update({\n",
    "                    'encoding_type': 'onehot',\n",
    "                    'encoded_features': feature_names.tolist(),\n",
    "                    'categories': encoder.categories_[0].tolist()\n",
    "                })\n",
    "    \n",
    "            else:\n",
    "                # Use LabelEncoder for all other categorical columns\n",
    "                encoder = LabelEncoder()\n",
    "                df_encoded[col] = encoder.fit_transform(df_encoded[col])\n",
    "    \n",
    "                # Store mapping information\n",
    "                encoding_info[col].update({\n",
    "                    'encoding_type': 'label',\n",
    "                    'mapping': dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "                })\n",
    "    \n",
    "            # Save each encoder individually\n",
    "            encoder_filename = os.path.join(save_path, f'{col}_encoder.pkl')\n",
    "            with open(encoder_filename, 'wb') as f:\n",
    "                pickle.dump(encoder, f)\n",
    "    \n",
    "            # Add encoder file path to encoding info\n",
    "            encoding_info[col]['encoder_path'] = encoder_filename\n",
    "    \n",
    "        # Add metadata information\n",
    "        encoding_info['metadata'] = {\n",
    "            'total_features_after_encoding': len(df_encoded.columns),\n",
    "            'ordinal_columns': ordinal_columns,\n",
    "            'nominal_columns': [col for col in self.categorical_cols if col not in ordinal_columns]\n",
    "        }\n",
    "    \n",
    "        return df_encoded, encoding_info\n",
    "    def advanced_missing_data_analysis(self):\n",
    "        # Create a heatmap of missing data\n",
    "        missing_matrix = self.df.isnull().astype(int)\n",
    "\n",
    "        # Create a heatmap\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=missing_matrix.T,  # Use the transposed matrix for the heatmap\n",
    "                x=list(missing_matrix.columns),  # Convert to list for x\n",
    "                y=list(missing_matrix.index),     # Convert to list for y\n",
    "                colorscale='Viridis',\n",
    "                colorbar=dict(title='Missing Value Indicator')\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(title='Missing Data Heatmap')\n",
    "        fig.show()\n",
    "        \n",
    "    def advanced_imputation(self):\n",
    "        \"\"\"Perform advanced imputation using multiple methods for both numerical and categorical data\"\"\"\n",
    "        df_imputed = {}\n",
    "        \n",
    "        # Separate numerical and categorical columns\n",
    "        numerical_cols = self.df.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = self.df.select_dtypes(exclude=['number']).columns\n",
    "        print(numerical_cols)\n",
    "        print(categorical_cols)\n",
    "        # 1. Iterative Imputer (MICE) for numerical data\n",
    "        \n",
    "        # imp_iter = IterativeImputer(\n",
    "        #     estimator=RandomForestRegressor(n_estimators=100),\n",
    "        #     max_iter=10,\n",
    "        #     random_state=42\n",
    "        # )\n",
    "        # df_num_imputed = pd.DataFrame(\n",
    "        #     imp_iter.fit_transform(self.df[numerical_cols]),\n",
    "        #     columns=numerical_cols\n",
    "        # )\n",
    "        \n",
    "        # Mode Imputer for categorical data\n",
    "        if (len(categorical_cols) >0):\n",
    "            imp_mode = SimpleImputer(strategy='most_frequent')\n",
    "            df_cat_imputed = pd.DataFrame(\n",
    "                imp_mode.fit_transform(self.df[categorical_cols]),\n",
    "                columns=categorical_cols\n",
    "            )\n",
    "        \n",
    "        # Combine numerical and categorical imputations\n",
    "        \n",
    "        # df_imputed['mice'] = pd.concat([df_num_imputed, df_cat_imputed], axis=1)\n",
    "    \n",
    "        # 2. KNN Imputer with optimal k for numerical data\n",
    "        #k_values = range(1, min(20, int(len(self.df)*0.001)))\n",
    "        k_values = range(1, 20)\n",
    "        scores = []\n",
    "    \n",
    "        for k in k_values:\n",
    "            imp_knn = KNNImputer(n_neighbors=k)\n",
    "            imputed = imp_knn.fit_transform(self.df[numerical_cols])\n",
    "            scores.append(np.mean(np.abs(imputed - self.df[numerical_cols].fillna(self.df[numerical_cols].mean()))))\n",
    "    \n",
    "        optimal_k = k_values[np.argmin(scores)]\n",
    "        imp_knn = KNNImputer(n_neighbors=optimal_k)\n",
    "        df_num_imputed_knn = pd.DataFrame(\n",
    "            imp_knn.fit_transform(self.df[numerical_cols]),\n",
    "            columns=numerical_cols\n",
    "        )\n",
    "        \n",
    "        # Reuse mode imputer results for categorical data in KNN version\n",
    "        if (len(categorical_cols) >0):\n",
    "            df_imputed['knn'] = pd.concat([df_num_imputed_knn, df_cat_imputed], axis=1)\n",
    "        df_imputed['knn'] = pd.concat([df_num_imputed_knn], axis=1)\n",
    "        # 3. Matrix Factorization for numerical data only (SVD)\n",
    "        \n",
    "        # U, s, Vh = np.linalg.svd(self.df[numerical_cols].fillna(0), full_matrices=False)\n",
    "        # rank = 3  # Can be adjusted based on explained variance\n",
    "        # df_num_imputed_svd = pd.DataFrame(\n",
    "        #     np.dot(U[:, :rank] * s[:rank], Vh[:rank, :]),\n",
    "        #     columns=numerical_cols\n",
    "        # )\n",
    "        \n",
    "        # # Combine SVD-imputed numerical data with mode-imputed categorical data\n",
    "        # df_imputed['svd'] = pd.concat([df_num_imputed_svd, df_cat_imputed], axis=1)\n",
    "    \n",
    "        self.df = df_imputed['knn'].copy()\n",
    "    def advanced_outlier_detection(self):\n",
    "        \"\"\"Perform advanced outlier detection\"\"\"\n",
    "        outliers = {}\n",
    "\n",
    "        # 1. Isolation Forest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        outliers['isolation_forest'] = iso_forest.fit_predict(self.df[self.numeric_cols])\n",
    "\n",
    "        # 2. Modified Z-score\n",
    "        def modified_zscore(x):\n",
    "            median = np.median(x)\n",
    "            mad = np.median(np.abs(x - median))\n",
    "            modified_zscores = 0.6745 * (x - median) / mad\n",
    "            return np.abs(modified_zscores) > 3.5\n",
    "\n",
    "        outliers['modified_zscore'] = self.df[self.numeric_cols].apply(modified_zscore)\n",
    "\n",
    "        # 3. Cook's Distance for regression\n",
    "        if self.target_col:\n",
    "\n",
    "\n",
    "            X = self.df[self.numeric_cols].drop(columns=[self.target_col])\n",
    "            y = self.df[self.target_col]\n",
    "            model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "            influence = OLSInfluence(model)\n",
    "            outliers['cooks_distance'] = influence.cooks_distance[0] > 4/len(self.df)\n",
    "\n",
    "        return outliers\n",
    "\n",
    "    def relationship_analysis(self):\n",
    "        return AdvancedRelationshipAnalysis.analyze_relationships(self.df)\n",
    "        \n",
    "    def dimensionality_reduction_analysis(self):\n",
    "        \"\"\"Perform advanced dimensionality reduction analysis\"\"\"\n",
    "        # Prepare data\n",
    "        scaler = RobustScaler()\n",
    "        scaled_data = scaler.fit_transform(self.df[self.numeric_cols])\n",
    "\n",
    "        # 1. PCA with explained variance analysis\n",
    "        pca = PCA()\n",
    "        pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "        # Explained variance plot\n",
    "        # Explained variance plot\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(1, len(pca.explained_variance_ratio_) + 1)),\n",
    "                y=np.cumsum(pca.explained_variance_ratio_),\n",
    "                mode='lines+markers',\n",
    "                name='Cumulative Explained Variance'\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            title='PCA Explained Variance Analysis',\n",
    "            xaxis_title='Number of Components',\n",
    "            yaxis_title='Cumulative Explained Variance Ratio'\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # 2. t-SNE visualization\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        tsne_result = tsne.fit_transform(scaled_data)\n",
    "\n",
    "        fig = go.Figure(data=go.Scatter(\n",
    "            x=tsne_result[:, 0],\n",
    "            y=tsne_result[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=self.df[self.target_col] if self.target_col else None,\n",
    "                colorscale='Viridis'\n",
    "            )\n",
    "        ))\n",
    "        fig.update_layout(title='t-SNE Visualization')\n",
    "        fig.show()\n",
    "\n",
    "        # 3. UMAP visualization\n",
    "        umap_model = umap.UMAP(random_state=42)\n",
    "        umap_result = umap_model.fit_transform(scaled_data)\n",
    "\n",
    "        fig = go.Figure(data=go.Scatter(\n",
    "            x=umap_result[:, 0],\n",
    "            y=umap_result[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=self.df[self.target_col] if self.target_col else None,\n",
    "                colorscale='Viridis'\n",
    "            )\n",
    "        ))\n",
    "        fig.update_layout(title='UMAP Visualization')\n",
    "        fig.show()\n",
    "\n",
    "    def advanced_categorical_analysis(self):\n",
    "        \"\"\"Perform advanced categorical data analysis\"\"\"\n",
    "        if len(self.categorical_cols) > 0:\n",
    "            # 1. Multiple Correspondence Analysis\n",
    "            mca = prince.MCA(n_components=2)\n",
    "            mca_coords = mca.fit_transform(self.df[self.categorical_cols])\n",
    "\n",
    "            fig = go.Figure(data=go.Scatter(\n",
    "                x=mca_coords[0],\n",
    "                y=mca_coords[1],\n",
    "                mode='markers',\n",
    "                text=self.df.index,\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "            fig.update_layout(title='Multiple Correspondence Analysis')\n",
    "            fig.show()\n",
    "\n",
    "            # 2. Advanced categorical encoding comparison\n",
    "            encoders = {\n",
    "                'target': TargetEncoder(),\n",
    "                'woe': WOEEncoder(),\n",
    "                'catboost': CatBoostEncoder()\n",
    "            }\n",
    "\n",
    "            encoded_dfs = {}\n",
    "            if self.target_col:\n",
    "                for name, encoder in encoders.items():\n",
    "                    encoded_dfs[name] = encoder.fit_transform(\n",
    "                        self.df[self.categorical_cols],\n",
    "                        self.df[self.target_col]\n",
    "                    )\n",
    "\n",
    "            # Compare encodings\n",
    "            if encoded_dfs:\n",
    "                for col in self.categorical_cols:\n",
    "                    fig = go.Figure()\n",
    "                    for name, encoded_df in encoded_dfs.items():\n",
    "                        fig.add_trace(\n",
    "                            go.Box(\n",
    "                                y=encoded_df[col],\n",
    "                                name=name,\n",
    "                                boxpoints='all'\n",
    "                            )\n",
    "                        )\n",
    "                    fig.update_layout(\n",
    "                        title=f'Encoding Comparison for {col}',\n",
    "                        yaxis_title='Encoded Values'\n",
    "                    )\n",
    "                    fig.show()\n",
    "\n",
    "    def advanced_distribution_analysis(self):\n",
    "        \"\"\"Perform advanced distribution analysis with interactive plots\"\"\"\n",
    "        distribution_plots = {}\n",
    "        for col in self.numeric_cols:\n",
    "            # Create distribution plot with multiple components\n",
    "            fig = make_subplots(rows=2, cols=2,\n",
    "                              subplot_titles=['Distribution', 'Q-Q Plot',\n",
    "                                            'Box Plot', 'Violin Plot'])\n",
    "            \n",
    "            data = self.df[col].dropna()\n",
    "            \n",
    "            # Histogram with optional KDE\n",
    "            try:\n",
    "                # Only attempt KDE if we have enough unique values\n",
    "                if len(data.unique()) > 1:\n",
    "                    # Add some small random noise if all values are integers\n",
    "                    if data.dtype.kind in 'iu' or (data % 1 == 0).all():\n",
    "                        data_for_kde = data + np.random.normal(0, 0.01, len(data))\n",
    "                    else:\n",
    "                        data_for_kde = data\n",
    "                        \n",
    "                    kde = stats.gaussian_kde(data_for_kde)\n",
    "                    x_range = np.linspace(min(data), max(data), 100)\n",
    "                    \n",
    "                    # Add histogram\n",
    "                    fig.add_trace(\n",
    "                        go.Histogram(x=data, name='Histogram',\n",
    "                                   nbinsx=50, histnorm='probability density'),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Add KDE curve\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=x_range, y=kde(x_range), name='KDE'),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "                else:\n",
    "                    # Fallback to simple histogram if KDE is not possible\n",
    "                    fig.add_trace(\n",
    "                        go.Histogram(x=data, name='Histogram',\n",
    "                                   nbinsx=50),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "            except (LinAlgError, ValueError) as e:\n",
    "                # Fallback to simple histogram if KDE fails\n",
    "                fig.add_trace(\n",
    "                    go.Histogram(x=data, name='Histogram',\n",
    "                               nbinsx=50),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "                \n",
    "            # Q-Q Plot with error handling\n",
    "            try:\n",
    "                qq = stats.probplot(data)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=qq[0][0], y=qq[0][1],\n",
    "                              mode='markers', name='Q-Q'),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "                # Add reference line\n",
    "                z = np.array([min(qq[0][0]), max(qq[0][0])])\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=z, y=z, mode='lines',\n",
    "                              name='Reference Line',\n",
    "                              line=dict(dash='dash')),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Add empty plot with error message if Q-Q plot fails\n",
    "                fig.add_annotation(text=\"Unable to create Q-Q plot\",\n",
    "                                 xref=\"x2\", yref=\"y2\",\n",
    "                                 x=0.5, y=0.5, showarrow=False)\n",
    "            \n",
    "            # Box Plot\n",
    "            fig.add_trace(\n",
    "                go.Box(y=data, name='Box Plot',\n",
    "                      boxpoints='outliers'),  # Show outliers\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Violin Plot with error handling\n",
    "            try:\n",
    "                fig.add_trace(\n",
    "                    go.Violin(y=data, name='Violin Plot',\n",
    "                             box_visible=True, meanline_visible=True,\n",
    "                             points='outliers'),  # Show outliers\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Fallback to box plot if violin plot fails\n",
    "                fig.add_trace(\n",
    "                    go.Box(y=data, name='Box Plot',\n",
    "                          boxpoints='outliers'),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            \n",
    "            # Update layout with additional statistical information\n",
    "            stats_text = (\n",
    "                f\"Mean: {data.mean():.2f}<br>\"\n",
    "                f\"Median: {data.median():.2f}<br>\"\n",
    "                f\"Std Dev: {data.std():.2f}<br>\"\n",
    "                f\"Skewness: {data.skew():.2f}<br>\"\n",
    "                f\"Kurtosis: {data.kurtosis():.2f}\"\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=800,\n",
    "                title=f'Distribution Analysis: {col}',\n",
    "                annotations=[\n",
    "                    dict(\n",
    "                        x=1.0,\n",
    "                        y=1.0,\n",
    "                        xref='paper',\n",
    "                        yref='paper',\n",
    "                        text=stats_text,\n",
    "                        showarrow=False,\n",
    "                        align='left'\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            distribution_plots[col] = fig\n",
    "            \n",
    "        return distribution_plots\n",
    "    def advanced_feature_relationships(self):\n",
    "        \"\"\"Analyze feature relationships using advanced techniques\"\"\"\n",
    "        feature_relationship_plots = {}\n",
    "    \n",
    "        # 1. Advanced Correlation Analysis\n",
    "        if len(self.numeric_cols) > 1:\n",
    "            # Spearman and Kendall correlations\n",
    "            correlations = {\n",
    "                'pearson': self.df[self.numeric_cols].corr(method='pearson'),\n",
    "                'spearman': self.df[self.numeric_cols].corr(method='spearman'),\n",
    "                'kendall': self.df[self.numeric_cols].corr(method='kendall')\n",
    "            }\n",
    "            # Plot correlation differences\n",
    "            for method, corr_matrix in correlations.items():\n",
    "                fig = go.Figure(data=go.Heatmap(\n",
    "                    z=corr_matrix,\n",
    "                    x=corr_matrix.columns,\n",
    "                    y=corr_matrix.columns,\n",
    "                    colorscale='RdBu'\n",
    "                ))\n",
    "                fig.update_layout(\n",
    "                    title=f'{method.capitalize()} Correlation Matrix',\n",
    "                    height=600\n",
    "                )\n",
    "                feature_relationship_plots[f'{method.capitalize()} Correlation Matrix'] = fig\n",
    "    \n",
    "        # 2. Non-linear Relationships\n",
    "        if len(self.numeric_cols) > 1:\n",
    "            for i, col1 in enumerate(self.numeric_cols[:-1]):\n",
    "                for col2 in self.numeric_cols[i+1:]:\n",
    "                    fig = go.Figure()\n",
    "                    \n",
    "                    # Remove rows with NaN or infinite values\n",
    "                    df_clean = self.df[[col1, col2]].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                    \n",
    "                    # Scatter plot\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=df_clean[col1],\n",
    "                            y=df_clean[col2],\n",
    "                            mode='markers',\n",
    "                            name='Data Points'\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    try:\n",
    "                        # Add LOWESS smoothing\n",
    "                        lowess = sm.nonparametric.lowess\n",
    "                        z = lowess(df_clean[col2], df_clean[col1])\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=z[:, 0],\n",
    "                                y=z[:, 1],\n",
    "                                name='LOWESS',\n",
    "                                line=dict(color='red')\n",
    "                            )\n",
    "                        )\n",
    "                    except FloatingPointError:\n",
    "                        print(f\"Floating point error in LOWESS for {col1} vs {col2}. Skipping LOWESS.\")\n",
    "    \n",
    "                    fig.update_layout(\n",
    "                        title=f'Non-linear Relationship: {col1} vs {col2}',\n",
    "                        xaxis_title=col1,\n",
    "                        yaxis_title=col2,\n",
    "                        height=600\n",
    "                    )\n",
    "                    feature_relationship_plots[f'{col1} vs {col2}'] = fig\n",
    "    \n",
    "        return feature_relationship_plots\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        # Create Sweetviz report\n",
    "        # report = sv.analyze(self.df)\n",
    "        # report.show_html('advanced_eda_report.html')\n",
    "    \n",
    "        # # Create DataPrep report\n",
    "        # dpeda_report = create_report(self.df)\n",
    "        # dpeda_report.show_browser()\n",
    "    \n",
    "        # Generate advanced distribution and feature relationship plots\n",
    "        distribution_plots = self.advanced_distribution_analysis()\n",
    "        feature_relationship_plots = self.advanced_feature_relationships()\n",
    "    \n",
    "        # Save the distribution plots to a separate HTML page\n",
    "        with open('distribution_plots.html', 'w') as f:\n",
    "            f.write(\"\"\"\n",
    "                <html>\n",
    "                    <head>\n",
    "                        <title>Distribution Plots</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "            \"\"\")\n",
    "            for plot_title, plot in distribution_plots.items():\n",
    "                f.write(f\"<h2>{plot_title}</h2>\")\n",
    "                f.write(plot.to_html(include_plotlyjs='cdn', full_html=False))\n",
    "            f.write(\"\"\"\n",
    "                    </body>\n",
    "                </html>\n",
    "            \"\"\")\n",
    "    \n",
    "        # Save the feature relationship plots to a separate HTML page\n",
    "        with open('feature_relationship_plots.html', 'w') as f:\n",
    "            f.write(\"\"\"\n",
    "                <html>\n",
    "                    <head>\n",
    "                        <title>Feature Relationship Plots</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "            \"\"\")\n",
    "            for plot_title, plot in feature_relationship_plots.items():\n",
    "                f.write(f\"<h2>{plot_title}</h2>\")\n",
    "                f.write(plot.to_html(include_plotlyjs='cdn', full_html=False))\n",
    "            f.write(\"\"\"\n",
    "                    </body>\n",
    "                </html>\n",
    "            \"\"\")\n",
    "    \n",
    "        # Summary statistics and insights\n",
    "        summary = {\n",
    "            'dataset_shape': self.df.shape,\n",
    "            'missing_percentages': (self.df.isnull().sum() / len(self.df) * 100).to_dict(),\n",
    "            'numeric_features': len(self.numeric_cols),\n",
    "            'categorical_features': len(self.categorical_cols),\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    \n",
    "        # Create the main report page\n",
    "        with open('advanced_eda_report.html', 'a') as f:\n",
    "            f.write(\"\"\"\n",
    "                <html>\n",
    "                    <head>\n",
    "                        <title>Advanced EDA Report</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "                        <h1>Advanced EDA Report</h1>\n",
    "                        <h2>Summary</h2>\n",
    "                        <pre>{}</pre>\n",
    "                        <h2>Distribution Plots</h2>\n",
    "                        <iframe src=\"distribution_plots.html\" width=\"100%\" height=\"2000\"></iframe>\n",
    "                        <h2>Feature Relationship Plots</h2>\n",
    "                        <iframe src=\"feature_relationship_plots.html\" width=\"100%\" height=\"2000\"></iframe>\n",
    "                    </body>\n",
    "                </html>\n",
    "            \"\"\".format(json.dumps(summary, indent=4)))\n",
    "    \n",
    "        return summary\n",
    "    # def run_all_eda(self):\n",
    "    #     self.advanced_missing_data_analysis()\n",
    "    #     self.advanced_imputation()\n",
    "    #     self.advanced_outlier_detection()\n",
    "    #     self.advanced_distribution_analysis()\n",
    "    #     self.advanced_feature_relationships()\n",
    "    #     self.dimensionality_reduction_analysis()\n",
    "    #     self.advanced_categorical_analysis()\n",
    "    #     self.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a926ef7-c4ff-44d1-80e1-9c44cb5e4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data.csv\")\n",
    "eda = AdvancedEDA(df, target_col=\"HasDetections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9648bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MachineIdentifier', 'ProductName', 'EngineVersion', 'AppVersion',\n",
      "       'AvSigVersion', 'IsBeta', 'RtpStateBitfield', 'IsSxsPassiveMode',\n",
      "       'DefaultBrowsersIdentifier', 'AVProductStatesIdentifier',\n",
      "       'AVProductsInstalled', 'AVProductsEnabled', 'HasTpm',\n",
      "       'CountryIdentifier', 'CityIdentifier', 'OrganizationIdentifier',\n",
      "       'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'Platform',\n",
      "       'Processor', 'OsVer', 'OsBuild', 'OsSuite', 'OsPlatformSubRelease',\n",
      "       'OsBuildLab', 'SkuEdition', 'IsProtected', 'AutoSampleOptIn', 'PuaMode',\n",
      "       'SMode', 'IeVerIdentifier', 'SmartScreen', 'Firewall', 'UacLuaenable',\n",
      "       'Census_MDC2FormFactor', 'Census_DeviceFamily',\n",
      "       'Census_OEMNameIdentifier', 'Census_OEMModelIdentifier',\n",
      "       'Census_ProcessorCoreCount', 'Census_ProcessorManufacturerIdentifier',\n",
      "       'Census_ProcessorModelIdentifier', 'Census_ProcessorClass',\n",
      "       'Census_PrimaryDiskTotalCapacity', 'Census_PrimaryDiskTypeName',\n",
      "       'Census_SystemVolumeTotalCapacity', 'Census_HasOpticalDiskDrive',\n",
      "       'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',\n",
      "       'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
      "       'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
      "       'Census_InternalPrimaryDisplayResolutionVertical',\n",
      "       'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',\n",
      "       'Census_InternalBatteryNumberOfCharges', 'Census_OSVersion',\n",
      "       'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSBuildNumber',\n",
      "       'Census_OSBuildRevision', 'Census_OSEdition', 'Census_OSSkuName',\n",
      "       'Census_OSInstallTypeName', 'Census_OSInstallLanguageIdentifier',\n",
      "       'Census_OSUILocaleIdentifier', 'Census_OSWUAutoUpdateOptionsName',\n",
      "       'Census_IsPortableOperatingSystem', 'Census_GenuineStateName',\n",
      "       'Census_ActivationChannel', 'Census_IsFlightingInternal',\n",
      "       'Census_IsFlightsDisabled', 'Census_FlightRing',\n",
      "       'Census_ThresholdOptIn', 'Census_FirmwareManufacturerIdentifier',\n",
      "       'Census_FirmwareVersionIdentifier', 'Census_IsSecureBootEnabled',\n",
      "       'Census_IsWIMBootEnabled', 'Census_IsVirtualDevice',\n",
      "       'Census_IsTouchEnabled', 'Census_IsPenCapable',\n",
      "       'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer',\n",
      "       'Wdft_RegionIdentifier', 'HasDetections'],\n",
      "      dtype='object')\n",
      "83\n",
      "Suppression des colonnes contenant 'id' dans leur nom...\n",
      "['MachineIdentifier', 'DefaultBrowsersIdentifier', 'AVProductStatesIdentifier', 'CountryIdentifier', 'CityIdentifier', 'OrganizationIdentifier', 'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'IeVerIdentifier', 'Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier', 'Census_ProcessorModelIdentifier', 'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 'Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier', 'Wdft_RegionIdentifier']\n",
      "Suppression des colonnes avec trop de valeurs manquantes...\n",
      "Suppression des colonnes avec une seule valeur unique...\n",
      "[]\n",
      "[]\n",
      "Suppression des colonnes avec une catégorie dominante...\n",
      "Suppression des colonnes avec faible variance...\n",
      "Suppression des colonnes fortement corrélées...\n",
      "Nettoyage des caractéristiques terminé.\n",
      "Colonnes restantes : ['EngineVersion', 'AppVersion', 'AvSigVersion', 'RtpStateBitfield', 'IsSxsPassiveMode', 'AVProductsInstalled', 'AVProductsEnabled', 'HasTpm', 'Processor', 'OsBuild', 'OsSuite', 'OsPlatformSubRelease', 'OsBuildLab', 'SkuEdition', 'IsProtected', 'SmartScreen', 'Firewall', 'UacLuaenable', 'Census_MDC2FormFactor', 'Census_ProcessorCoreCount', 'Census_PrimaryDiskTotalCapacity', 'Census_PrimaryDiskTypeName', 'Census_SystemVolumeTotalCapacity', 'Census_HasOpticalDiskDrive', 'Census_TotalPhysicalRAM', 'Census_ChassisTypeName', 'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical', 'Census_PowerPlatformRoleName', 'Census_InternalBatteryNumberOfCharges', 'Census_OSVersion', 'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSBuildNumber', 'Census_OSBuildRevision', 'Census_OSEdition', 'Census_OSSkuName', 'Census_OSInstallTypeName', 'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName', 'Census_ActivationChannel', 'Census_FlightRing', 'Census_IsSecureBootEnabled', 'Census_IsTouchEnabled', 'Census_IsPenCapable', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer', 'HasDetections']\n",
      "RtpStateBitfield\n",
      "IsSxsPassiveMode\n",
      "AVProductsInstalled\n",
      "AVProductsEnabled\n",
      "HasTpm\n",
      "OsBuild\n",
      "OsSuite\n",
      "IsProtected\n",
      "Firewall\n",
      "UacLuaenable\n",
      "Census_ProcessorCoreCount\n",
      "Census_PrimaryDiskTotalCapacity\n",
      "Census_SystemVolumeTotalCapacity\n",
      "Census_HasOpticalDiskDrive\n",
      "Census_TotalPhysicalRAM\n",
      "Census_InternalPrimaryDiagonalDisplaySizeInInches\n",
      "Census_InternalPrimaryDisplayResolutionHorizontal\n",
      "Census_InternalPrimaryDisplayResolutionVertical\n",
      "Census_InternalBatteryNumberOfCharges\n",
      "Census_OSBuildNumber\n",
      "Census_OSBuildRevision\n",
      "Census_IsSecureBootEnabled\n",
      "Census_IsTouchEnabled\n",
      "Census_IsPenCapable\n",
      "Census_IsAlwaysOnAlwaysConnectedCapable\n",
      "Wdft_IsGamer\n",
      "HasDetections\n",
      "EngineVersion\n",
      "AppVersion\n",
      "AvSigVersion\n",
      "Processor\n",
      "OsPlatformSubRelease\n",
      "OsBuildLab\n",
      "SkuEdition\n",
      "SmartScreen\n",
      "Census_MDC2FormFactor\n",
      "Census_PrimaryDiskTypeName\n",
      "Census_ChassisTypeName\n",
      "Census_PowerPlatformRoleName\n",
      "Census_OSVersion\n",
      "Census_OSArchitecture\n",
      "Census_OSBranch\n",
      "Census_OSEdition\n",
      "Census_OSSkuName\n",
      "Census_OSInstallTypeName\n",
      "Census_OSWUAutoUpdateOptionsName\n",
      "Census_GenuineStateName\n",
      "Census_ActivationChannel\n",
      "Census_FlightRing\n"
     ]
    }
   ],
   "source": [
    "eda.simple_statistical_imputation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d2bf09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean_df_cols.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(eda.df.columns,\"clean_df_cols.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699a093-9ed5-4d7b-abbf-ed446aed3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.df.to_csv(\"./data_clean_0.csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e7ec26-cace-4242-add6-a596572f53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded, _ = eda.advanced_categorical_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27bbe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(\"HasDetections\",axis = 1)\n",
    "cols = X.columns\n",
    "sc = StandardScaler()\n",
    "X = pd.DataFrame(sc.fit_transform(X) , columns=cols)\n",
    "joblib.dump(sc,\"StandardScaler.pkl\")\n",
    "X_train , X_test , y_train , y_test = train_test_split(X ,df_encoded.HasDetections , test_size = 0.2 , stratify=df_encoded.HasDetections )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI']=\"https://dagshub.com/yahiabouzaouach/mlops_project.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME']= 'bilel9999'\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = '4a3b567f61acebf8c9bcd0cd17a4ce8d0e0a7cca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6828dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('https://dagshub.com/yahiabouzaouach/mlops_project.mlflow') #your mlfow tracking uri\n",
    "mlflow.set_experiment(\"Malware_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='LogisticRegression'):\n",
    "    # mlflow.log_param(\"data_url\",data_url)\n",
    "    mlflow.log_param(\"data_version\",0)\n",
    "    mlflow.log_param(\"input_rows\",eda.df.shape[0])\n",
    "    mlflow.log_param(\"input_cols\",eda.df.shape[1])\n",
    "    #model fitting and training\n",
    "    lr=LogisticRegression()\n",
    "    mlflow.set_tag(key= \"model\",value=\"LogisticRegression\")\n",
    "    params = lr.get_params()\n",
    "    mlflow.log_params(params)\n",
    "    lr.fit(X_train,y_train)\n",
    "    train_features_name = f'{X_train=}'.split('=')[0]\n",
    "    train_label_name = f'{y_train=}'.split('=')[0]\n",
    "    mlflow.set_tag(key=\"train_features_name\",value= train_features_name)\n",
    "    mlflow.set_tag(key= \"train_label_name\",value=train_label_name)\n",
    "    predicted=lr.predict(X_test)\n",
    "    precision,recall,fscore,support=score(y_test,predicted,average='macro')\n",
    "    mlflow.log_metric(\"Precision_test\",precision)\n",
    "    mlflow.log_metric(\"Recall_test\",recall)\n",
    "    mlflow.log_metric(\"F1_score_test\",fscore)\n",
    "    mlflow.sklearn.log_model(lr,artifact_path=\"ML_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb88ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='RandomForestClassifier'):\n",
    "    # mlflow.log_param(\"data_url\",data_url)\n",
    "    mlflow.log_param(\"data_version\",0)\n",
    "    mlflow.log_param(\"input_rows\",eda.df.shape[0])\n",
    "    mlflow.log_param(\"input_cols\",eda.df.shape[1])\n",
    "    #model fitting and training\n",
    "    rf= RandomForestClassifier()\n",
    "    mlflow.set_tag(key= \"model\",value=\"RandomForestClassifier\")\n",
    "    params = rf.get_params()\n",
    "    mlflow.log_params(params)\n",
    "    rf.fit(X_train,y_train)\n",
    "    train_features_name = f'{X_train=}'.split('=')[0]\n",
    "    train_label_name = f'{y_train=}'.split('=')[0]\n",
    "    mlflow.set_tag(key=\"train_features_name\",value= train_features_name)\n",
    "    mlflow.set_tag(key= \"train_label_name\",value=train_label_name)\n",
    "    predicted=lr.predict(X_test)\n",
    "    precision,recall,fscore,support=score(y_test,predicted,average='macro')\n",
    "    mlflow.log_metric(\"Precision_test\",precision)\n",
    "    mlflow.log_metric(\"Recall_test\",recall)\n",
    "    mlflow.log_metric(\"F1_score_test\",fscore)\n",
    "    mlflow.sklearn.log_model(rf,artifact_path=\"ML_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "    'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 0.3),\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 31, 200),\n",
    "    'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "    'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "    'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),\n",
    "    'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
    "    'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "    'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "    'device': 'gpu',  # Use GPU\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    }\n",
    "\n",
    "    # Create pipeline with model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', lgb.LGBMClassifier(**params, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    try:\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='roc_auc'\n",
    "        )\n",
    "        return np.mean(cv_scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trial: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)  # Reduced trials for quicker demonstration\n",
    "\n",
    "# Print best trial information\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value: ', trial.value)\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_params = trial.params\n",
    "best_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', lgb.LGBMClassifier(**best_params, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='LightGBM'):\n",
    "    mlflow.log_param(\"data_version\",0)\n",
    "    mlflow.log_param(\"input_rows\",eda.df.shape[0])\n",
    "    mlflow.log_param(\"input_cols\",eda.df.shape[1])\n",
    "    #model fitting and training\n",
    "    mlflow.set_tag(key= \"model\",value=\"LightGBM\")\n",
    "    params = best_pipeline.get_params()\n",
    "    mlflow.log_params(params)\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    train_features_name = f'{X_train=}'.split('=')[0]\n",
    "    train_label_name = f'{y_train=}'.split('=')[0]\n",
    "    mlflow.set_tag(key=\"train_features_name\",value= train_features_name)\n",
    "    mlflow.set_tag(key= \"train_label_name\",value=train_label_name)\n",
    "    predicted=best_pipeline.predict_proba(X_test)[:, 1]\n",
    "    binary_predictions = (predicted >= 0.5).astype(int)\n",
    "    mlflow.log_metric(\"Precision_test\",precision_score(y_test, binary_predictions))\n",
    "    mlflow.log_metric(\"Recall_test\",recall_score(y_test, binary_predictions))\n",
    "    mlflow.log_metric(\"F1_score_test\",f1_score(y_test, binary_predictions))\n",
    "    mlflow.log_metric(\"roc_auc_score\", roc_auc_score(y_test, binary_predictions))\n",
    "    mlflow.sklearn.log_model(best_pipeline,artifact_path=\"ML_models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
